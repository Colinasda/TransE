# TransE

translation embedding模型之TransE

该模型的基本思想是使head向量和relation向量的和尽可能靠近tail向量。这里我们用L1或L2范数来衡量它们的靠近程度。

![](https://tva1.sinaimg.cn/large/0081Kckwly1gm3x05c98tj30f70d9t9w.jpg)


## Pseudo code

![](https://tva1.sinaimg.cn/large/008i3skNgy1gqjd2jkc01j30ke0ad763.jpg)

理解下来也就是：

* 首先，确定训练集，超参数γ，学习率λ
* 初始化关系向量与实体向量，对于每个向量的每个维度在 [ -6/√k，6/√k）内随机取一个值，k为低维向量的维数，对所有的向量初始化之后要进行归一化
* 进入循环：采用minibatch，一批一批的训练会加快训练速度，对于每批数据进行负采样（将训练集中的三元组某一实体随机替换掉），T_batch初始为一个空列表，然后向其添加由元组对（原三元组，打碎的三元组）组成的列表 ：
  T_batch = [ ( [h,r,t], [h',r,t'] ), （[ ], [ ]）, ......]
* 拿到T_batch后进行训练，采用梯度下降进行调参

### 1 loss function

TransE 定义了一个距离函数 d(h + r, t)，它用来衡量 h + r 和 t 之间的距离，在实际应用中可以使用 **L1** 或 **L2** 范数。在模型的训练过程中，transE采用**最大间隔**方法，最小化目标函数，目标函数如下：

![](https://tva1.sinaimg.cn/large/008i3skNgy1gqjdpbbgs0j30c901q74e.jpg)

其中，S是知识库中的三元组即训练集，S’是负采样的三元组，通过替换 h 或 t 所得，是人为随机生成的。γ 是取值大于0的间隔距离参数，是一个超参数，[x]+表示正值函数，即 x > 0时，[x]+ = x；当 x ≤ 0 时，[x]+ = 0 。算法模型比较简单，梯度更新只需计算距离 d(h+r, t) 和 d(h’+r, t’)。


### 2 归一化

数据归一化是指数据减去对应维度的最小值除以维度最大值减去维度最小值，这样做可以将数值压缩到[0,1]的区间。

公式：(xi−min(x))/(max(x)−min(x))



### 3 梯度下降

#### 3.1批量梯度下降法（Batch Gradient Descent）

批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用**所有的样本**来进行更新。　

#### 3.2随机梯度下降法（Stochastic Gradient Descent）

随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。

随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

> 这里transE使用的是SGD，SGD的收敛没有GD好，但是，这反而是优点，因为在机器学习领域，过于best的结果反而是害处，因为用于过拟合（overfitting）

#### 3.3小批量梯度下降法（Mini-batch Gradient Descent）

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1<x<m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。

***

数据集包括：

YAGO3-10

FB15K

WN18

umls
